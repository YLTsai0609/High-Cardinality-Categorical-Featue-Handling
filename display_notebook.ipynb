{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T09:59:07.727537Z",
     "start_time": "2019-08-11T09:58:58.337053Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "# set seed and path\n",
    "SEED = 17\n",
    "ROOT = Path('.')\n",
    "TRAIN_FILE = ROOT / 'data/train.csv'\n",
    "\n",
    "# check your current working directory\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T09:59:07.925388Z",
     "start_time": "2019-08-11T09:59:07.736676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RESOURCE            int64\n",
       "MGR_ID              int64\n",
       "ROLE_FAMILY_DESC    int64\n",
       "ROLE_FAMILY         int64\n",
       "ROLE_CODE           int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(32769, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RESOURCE</th>\n",
       "      <th>MGR_ID</th>\n",
       "      <th>ROLE_FAMILY_DESC</th>\n",
       "      <th>ROLE_FAMILY</th>\n",
       "      <th>ROLE_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39353</td>\n",
       "      <td>85475</td>\n",
       "      <td>117906</td>\n",
       "      <td>290919</td>\n",
       "      <td>117908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17183</td>\n",
       "      <td>1540</td>\n",
       "      <td>118536</td>\n",
       "      <td>308574</td>\n",
       "      <td>118539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36724</td>\n",
       "      <td>14457</td>\n",
       "      <td>267952</td>\n",
       "      <td>19721</td>\n",
       "      <td>117880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36135</td>\n",
       "      <td>5396</td>\n",
       "      <td>240983</td>\n",
       "      <td>290919</td>\n",
       "      <td>118322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42680</td>\n",
       "      <td>5905</td>\n",
       "      <td>123932</td>\n",
       "      <td>19793</td>\n",
       "      <td>119325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RESOURCE  MGR_ID  ROLE_FAMILY_DESC  ROLE_FAMILY  ROLE_CODE\n",
       "0     39353   85475            117906       290919     117908\n",
       "1     17183    1540            118536       308574     118539\n",
       "2     36724   14457            267952        19721     117880\n",
       "3     36135    5396            240983       290919     118322\n",
       "4     42680    5905            123932        19793     119325"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'TARGET'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    30872\n",
       "0     1897\n",
       "Name: ACTION, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_FILE)\n",
    "y = train['ACTION']\n",
    "train = train[['RESOURCE', 'MGR_ID',\n",
    "               'ROLE_FAMILY_DESC', 'ROLE_FAMILY',\n",
    "               'ROLE_CODE']]\n",
    "display(train.dtypes,\n",
    "       train.shape,\n",
    "       train.head(),\n",
    "       'TARGET',\n",
    "       y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data 簡介\n",
    "這次的示範資料集是從Kaggle上2013年的[Amazon員工訪問權限預測挑戰賽](https://www.kaggle.com/c/amazon-employee-access-challenge)中取得\n",
    "這個資料集，該資料集收集了Amazon公司中各個員工針對每個資源(例如網頁的logging)的訪問紀錄，當員工屬於能夠取得訪問權限時，系統卻不給訪問，又要向上申請才能取得權限，一來一往浪費的非常多時間，因此這場比賽希望能夠建構模型，減少員工訪問權限所需的人工流程，我們取出5個特徵如下 :\n",
    "\n",
    "\n",
    "* Feature (X)\n",
    "\n",
    "> RESOURCE : 資源ID\n",
    "\n",
    "> MGR_ID : 員工主管的ID \n",
    "\n",
    "> ROLE_FAMILY_DESC : 員工類別擴展描述 (例如 軟體工程的零售經理)\n",
    "\n",
    "> ROLE_FAMILY : 員工類別 (例如 零售經理)\n",
    "\n",
    "> ROLE_CODE : 員工角色編碼 (例如 經理)\n",
    "\n",
    "* Target (Y)\n",
    "\n",
    "> ACTION : \n",
    "\n",
    " >> 1 : RESOURCE 訪問權限取得\n",
    " \n",
    " >> 0 : RESOURCE 禁止訪問"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T09:59:07.948193Z",
     "start_time": "2019-08-11T09:59:07.930886Z"
    }
   },
   "outputs": [],
   "source": [
    "# helper function\n",
    "def get_score(model, X, y, X_val, y_val):\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict_proba(X_val)[:,1]\n",
    "    score = roc_auc_score(y_val, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T09:59:07.978352Z",
     "start_time": "2019-08-11T09:59:07.958602Z"
    }
   },
   "outputs": [],
   "source": [
    "LogReg_model = LogisticRegression(random_state=SEED)\n",
    "xgb_model = XGBClassifier(random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train, y, test_size=0.2, random_state=SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T09:59:08.015835Z",
     "start_time": "2019-08-11T09:59:07.983767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESOURCE uniques :  6711\n",
      "MGR_ID uniques :  4062\n",
      "ROLE_FAMILY_DESC uniques :  2201\n",
      "ROLE_FAMILY uniques :  67\n",
      "ROLE_CODE uniques :  337\n"
     ]
    }
   ],
   "source": [
    "for col in X_train.columns:\n",
    "    print(col , 'uniques : ',X_train[f'{col}'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T09:59:10.951237Z",
     "start_time": "2019-08-11T09:59:08.024012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Label encoding: 0.5350998573215817\n",
      "Xgboost Label encoding: 0.7271242303799603\n"
     ]
    }
   ],
   "source": [
    "baseline_logit_score = get_score(LogReg_model, X_train, y_train, X_val, y_val)\n",
    "baseline_xgb_score = get_score(xgb_model, X_train, y_train, X_val, y_val)\n",
    "print('Logistic Regression Label encoding:', baseline_logit_score)\n",
    "print('Xgboost Label encoding:', baseline_xgb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T09:59:11.124628Z",
     "start_time": "2019-08-11T09:59:10.970715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 5\n",
      "Features after OHE 13378\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_enc = OneHotEncoder(sparse=True)\n",
    "\n",
    "print('Original number of features:', X_train.shape[1])\n",
    "data_ohe_train = (one_hot_enc.fit_transform(X_train))\n",
    "data_ohe_val = (one_hot_enc.transform(X_val))\n",
    "print('Features after OHE', data_ohe_train.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T09:59:23.751215Z",
     "start_time": "2019-08-11T09:59:11.137931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression OneHot encoding: 0.8658363819423944\n",
      "Xgboost OneHot encoding: 0.7223277228383869\n"
     ]
    }
   ],
   "source": [
    "ohe_logit_score = get_score(LogReg_model, data_ohe_train, y_train, data_ohe_val, y_val)\n",
    "ohe_xgb_score = get_score(xgb_model, data_ohe_train, y_train, data_ohe_val, y_val)\n",
    "print('Logistic Regression OneHot encoding:', ohe_logit_score)\n",
    "print('Xgboost OneHot encoding:', ohe_xgb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T09:59:23.792166Z",
     "start_time": "2019-08-11T09:59:23.757446Z"
    }
   },
   "outputs": [],
   "source": [
    "def TargetEncoder(train, test, ft, target, \n",
    "                   min_samples_leaf=1,\n",
    "                   smoothing_slope=1,\n",
    "                   noise_level=0,\n",
    "                   handle_missing='informed', handle_unseen='overall_mean',\n",
    "                   verbose=True):\n",
    "    '''\n",
    "        Tree model 處理 high cardinality特徵的方法, (例如, 地區, 地址, IP...)\n",
    "        由於特徵非線性且基數高，導致Tree model非常容易overfitting，\n",
    "        Target encoding的中心思想為 :\n",
    "        將類別特徵轉換為數值特徵，使用該特徵中每個種類的sooth_mean，\n",
    "        smooth_mean可以理解為，當該種類出現的次數越多次，我們就越相信該平均值，否則資訊量太少，\n",
    "        我們傾向相信總平均值。\n",
    "        公式為 : smooth_mean = smoothing_factor * estimated_mean + (1 - smoothing_factor) * overall mean\n",
    "        其中 smoothing_factor =  1 / (1 + np.exp(-(counts - min_samples_leaf) / smoothing_slope))\n",
    "        when min_samples_leaf, smoothing_slope fixed, counts -> infinity, smoothing_factor -> 1\n",
    "        min_sample_leaf 為曲線的反曲點, 當counts = min_sample_leaf 時， smoothing_factor = 0.5\n",
    "        smoothing_slope 為曲線從反曲點趨近於0和1的增加量 :\n",
    "        當smoothing_slope -> infinity, smoothing_factor = 0.5\n",
    "        當smoothing_slope -> 0 smoothing_factor -> step function\n",
    "        \n",
    "    :param train: pd.DataFrame\n",
    "    :param test: pd.DataFrame \n",
    "    :param ft: string \n",
    "    :param target : pd.Series with target name\n",
    "    :param noise_level: float  noise level\n",
    "    :param handle_missing: string 'overall_mean','informed'\n",
    "    :param handle_unseen: string 'overall_mean','return_nan'\n",
    "    :param verbose: bool, check the unseen in testing set\n",
    "    :return: train - pd.Series, test, - pd.Series target encoding result\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def add_noise(s: 'pd.Series', noise_level: int) -> 'pd.Series':\n",
    "        return s * (1 + noise_level * np.random.randn(len(s)))\n",
    "    \n",
    "    train = pd.concat([train, target], axis=1)\n",
    "    target_name = target.name\n",
    "    overall_mean = target.mean()\n",
    "\n",
    "    # handling missing value - filling\n",
    "    train[ft].fillna('is_missing')\n",
    "    test[ft].fillna('is_missing')\n",
    "\n",
    "    # Compute the number of values and the estimated_mean of each group\n",
    "    agg = train.groupby(ft)[target_name].agg(['count', 'mean'])\n",
    "    counts = agg['count']\n",
    "    estimated_mean = agg['mean']\n",
    "\n",
    "    if handle_missing == 'overall_mean':\n",
    "        # assign zero to group \"is_missing\", then smooth will be overall_mean\n",
    "        counts.is_missing = 0\n",
    "\n",
    "    # Compute the \"smoothed\" overall_means\n",
    "    # DEFAULT take missing value is informed\n",
    "\n",
    "    smoothing_factor = 1 / (1 + np.exp(- (counts - min_samples_leaf) / smoothing_slope))\n",
    "    smooth_mean = (smoothing_factor * estimated_mean + (1 - smoothing_factor) * overall_mean)\n",
    "\n",
    "    # create seen variable for test\n",
    "    test_seen = test[ft].map(smooth_mean)\n",
    "    unseen_ratio = test_seen.isnull().sum() / len(test)\n",
    "\n",
    "    # return nan for unseen variable\n",
    "    if handle_unseen == 'return_nan':\n",
    "        return add_noise(train[ft].map(smooth_mean), noise_level), add_noise(test_seen, noise_level)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'feature \"{ft}\" overall_mean is : ', round(overall_mean, 3))\n",
    "        print(f'feature \"{ft}\" unssen ratio in test set is : ', round(unseen_ratio, 3))\n",
    "\n",
    "    # DEFAULT return overall_mean for unseen variable\n",
    "    return add_noise(train[ft].map(smooth_mean), noise_level), add_noise(test_seen.fillna(overall_mean), noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T09:59:24.028859Z",
     "start_time": "2019-08-11T09:59:23.799720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature \"RESOURCE\" overall_mean is :  0.942\n",
      "feature \"RESOURCE\" unssen ratio in test set is :  0.132\n",
      "feature \"MGR_ID\" overall_mean is :  0.942\n",
      "feature \"MGR_ID\" unssen ratio in test set is :  0.032\n",
      "feature \"ROLE_FAMILY_DESC\" overall_mean is :  0.942\n",
      "feature \"ROLE_FAMILY_DESC\" unssen ratio in test set is :  0.028\n",
      "feature \"ROLE_FAMILY\" overall_mean is :  0.942\n",
      "feature \"ROLE_FAMILY\" unssen ratio in test set is :  0.0\n",
      "feature \"ROLE_CODE\" overall_mean is :  0.942\n",
      "feature \"ROLE_CODE\" unssen ratio in test set is :  0.001\n"
     ]
    }
   ],
   "source": [
    "data_te_train = X_train.copy()\n",
    "data_te_val = X_val.copy()\n",
    "for feature in data_te_train.columns:\n",
    "    data_te_train[f'{feature}_te'], data_te_val[f'{feature}_te'] = TargetEncoder(data_te_train,\n",
    "                                                                           X_val, ft=feature,\n",
    "                                                                           target=y_train,\n",
    "                                                                      min_samples_leaf=3, smoothing_slope=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T09:59:27.014285Z",
     "start_time": "2019-08-11T09:59:24.033846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Target encoding: 0.8246538027664151\n",
      "Xgboost Target encoding: 0.8289567782791435\n"
     ]
    }
   ],
   "source": [
    "te_col = [feature for feature in data_te_train.columns\n",
    "                              if feature.endswith('_te')]\n",
    "# Training\n",
    "target_logit_score = get_score(LogReg_model, data_te_train[te_col], y_train, data_te_val[te_col], y_val)\n",
    "target_xgb_score = get_score(xgb_model, data_te_train[te_col], y_train, data_te_val[te_col], y_val)\n",
    "print('Logistic Regression Target encoding:', target_logit_score)\n",
    "print('Xgboost Target encoding:', target_xgb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T10:00:40.914040Z",
     "start_time": "2019-08-11T10:00:33.847581Z"
    }
   },
   "outputs": [],
   "source": [
    "FOLDS = StratifiedKFold(n_splits=5, random_state=SEED)\n",
    "VAL_FOLDS = KFold(n_splits=5, random_state=SEED)\n",
    "\n",
    "data_te_cv_train = X_train.copy()\n",
    "data_te_cv_val = X_val.copy()\n",
    "oof_trn_list = []\n",
    "oof_test_list = []\n",
    "\n",
    "for (trn_idx, val_idx), (_, test_idx) in zip(FOLDS.split(data_te_cv_train, y_train),\n",
    "                                             VAL_FOLDS.split(data_te_cv_val)):\n",
    "    X_train_te, y_train_te = data_te_cv_train.iloc[trn_idx], y_train.iloc[trn_idx]\n",
    "    X_val_te, y_val_te = data_te_cv_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "    X_test_te = data_te_cv_val.iloc[test_idx]\n",
    "    \n",
    "    for feature in X_train_te.columns:\n",
    "        X_train_te[f'{feature}_te'], X_val_te[f'{feature}_te'] = TargetEncoder(X_train_te,\n",
    "                                                                               X_val_te,\n",
    "                                                                               ft=feature,\n",
    "                                                                               target=y_train_te,\n",
    "                                                                               min_samples_leaf=3,\n",
    "                                                                               smoothing_slope=2,\n",
    "                                                                               verbose=False)\n",
    "        _, X_test_te[f'{feature}_te'] =TargetEncoder(X_train_te,\n",
    "                                                    X_test_te,\n",
    "                                                    ft=feature,\n",
    "                                                    target=y_train_te,\n",
    "                                                    min_samples_leaf=3,\n",
    "                                                    smoothing_slope=2,\n",
    "                                                    verbose=False)\n",
    "    \n",
    "    oof_trn_list.append(X_val_te.reset_index(drop=True))\n",
    "    oof_test_list.append(X_test_te.reset_index(drop=True))\n",
    "\n",
    "te_col = [feature for feature in data_te_train.columns\n",
    "                              if feature.endswith('_te')]\n",
    "\n",
    "data_te_cv_train = pd.concat(oof_trn_list,ignore_index=True)\n",
    "data_te_cv_val = pd.concat(oof_test_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T10:00:35.909Z"
    }
   },
   "outputs": [],
   "source": [
    "te_cv_logit_score = get_score(LogReg_model, data_te_cv_train[te_col], y_train, data_te_cv_val[te_col], y_val)\n",
    "te_cv_xgb_score = get_score(xgb_model, data_te_cv_train[te_col], y_train, data_te_cv_val[te_col], y_val)\n",
    "print('Logistic Regression Regularized Target encoding:', te_cv_logit_score)\n",
    "print('Xgboost Regularized Target encoding:', te_cv_xgb_score)\n",
    "# # Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T10:00:36.621Z"
    }
   },
   "outputs": [],
   "source": [
    "data_embedding_train = X_train.copy()\n",
    "data_embedding_val = X_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T10:00:36.986Z"
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingMapping():\n",
    "    \"\"\"\n",
    "    Helper class for handling categorical variables\n",
    "    An instance of this class should be defined for each categorical variable we want to use.\n",
    "    \"\"\"\n",
    "    def __init__(self, s : 'pd.Series') -> None:\n",
    "        values = s.unique().tolist()\n",
    "        self.feature_name = s.name\n",
    "        # dictionary mapper\n",
    "        self.embedding_dict = {value: int_value + 1 for int_value, value in enumerate(values)}\n",
    "        \n",
    "        # The num_values will be used as the input_dim when defining the embedding layer.\n",
    "        # we set unseen values as maximum value + 1 \n",
    "        self.num_values = len(values) + 1\n",
    "\n",
    "    def mapping(self,s : 'pd.Series', verbose = True) -> None:\n",
    "        tmp_series = s.map(self.embedding_dict)\n",
    "        unseen_ratio = round(tmp_series.isnull().sum() / len(s), 3)\n",
    "        if verbose:\n",
    "            print(f'Feature \"{self.feature_name}\"')\n",
    "            print(f'    encode {self.num_values -1} values to label, {self.num_values} will be the unseen value ')\n",
    "            print(f'    unssen ratio is : ', unseen_ratio)\n",
    "        return tmp_series.fillna(self.num_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T10:00:37.234Z"
    }
   },
   "outputs": [],
   "source": [
    "for feature in data_embedding_train.columns:\n",
    "    Mapper = EmbeddingMapping(data_embedding_train[feature])\n",
    "    data_embedding_train[f'{feature}'] = Mapper.mapping(data_embedding_train[f'{feature}'])\n",
    "    data_embedding_val[f'{feature}'] = Mapper.mapping(data_embedding_val[f'{feature}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T10:00:37.502Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "LR = .00005\n",
    "EPOCHS = 80\n",
    "hidden_units = (16,4)\n",
    " \n",
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    keras.backend.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "# Google's paper tell us a good herustic size = number of category ** 0.25\n",
    "embedding_size_dict = {'RESOURCE': 9,\n",
    "                       'MGR_ID': 8 ,\n",
    "                      'ROLE_FAMILY_DESC': 7,\n",
    "                      'ROLE_FAMILY': 3,\n",
    "                      'ROLE_CODE':  4,\n",
    "                      }\n",
    "\n",
    "def build_and_train_model(df, target,  \n",
    "                          verbose, hidden_units, epochs, LR,\n",
    "                         embedding_size_dict, SEED):\n",
    "    tf.set_random_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "    def build_input_and_embedding_layer(s, embedding_size_dict = embedding_size_dict):\n",
    "        assert s.name in embedding_size_dict.keys()\n",
    "        input_layer = keras.Input(shape=(1,), name = s.name)\n",
    "        embedded_layer = keras.layers.Embedding(s.max() + 1, \n",
    "                                               embedding_size_dict[s.name],\n",
    "                                               input_length=1, name = f'{s.name}_embedding')(input_layer)\n",
    "        return input_layer, embedded_layer\n",
    "    # Create embedding layer\n",
    "    emb_layer_list = []\n",
    "    input_layer_list = []\n",
    "    for feature in df.columns:\n",
    "        input_layer, embedded_layer = build_input_and_embedding_layer(df[feature], embedding_size_dict=embedding_size_dict)\n",
    "        input_layer_list.append(input_layer)\n",
    "        emb_layer_list.append(embedded_layer)\n",
    "    # concat\n",
    "    concatenated = keras.layers.Concatenate()(emb_layer_list)\n",
    "    out = keras.layers.Flatten()(concatenated)\n",
    "    \n",
    "    # hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "        out = keras.layers.Dense(n_hidden, activation='relu')(out)\n",
    "        out = Dropout(0.3)(out)\n",
    "    # output layer\n",
    "    out = keras.layers.Dense(1, activation='sigmoid', name='prediction')(out)\n",
    "    \n",
    "    # model\n",
    "    model = keras.Model(\n",
    "    inputs = input_layer_list,\n",
    "    outputs = out)\n",
    "#     model.summary()\n",
    "\n",
    "    model.compile(\n",
    "    tf.train.AdamOptimizer(LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[auc],\n",
    "    )\n",
    "    tf.initialize_all_variables()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    history = model.fit(\n",
    "    [df[feature] for feature in df.columns],\n",
    "    target,\n",
    "    batch_size= 32,\n",
    "    callbacks=[callback],\n",
    "    epochs=epochs,\n",
    "    verbose=verbose,\n",
    "    validation_split = .1\n",
    "    )\n",
    "    return history\n",
    "\n",
    "history = build_and_train_model(df = data_embedding_train,\n",
    "                      target = y_train,\n",
    "                      verbose=2, \n",
    "                      hidden_units=hidden_units,\n",
    "                      epochs=EPOCHS,\n",
    "                      LR=LR,\n",
    "                      embedding_size_dict = embedding_size_dict,\n",
    "                      SEED=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T10:00:37.704Z"
    }
   },
   "outputs": [],
   "source": [
    "history_FS = (15, 5)\n",
    "def plot_history(histories, keys=('loss','auc',), train=True, figsize=history_FS):\n",
    "    if isinstance(histories, tf.keras.callbacks.History):\n",
    "        histories = [ ('', histories) ]\n",
    "        print(histories, type(histories))\n",
    "    print(histories, type(histories))\n",
    "    for key in keys:\n",
    "        plt.figure(figsize=history_FS)\n",
    "        for name, history in histories:\n",
    "            val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                           '--', label=str(name).title()+' Val')\n",
    "            if train:\n",
    "                plt.plot(history.epoch, history.history[key], color=val[0].get_color(), alpha=.5,\n",
    "                         label=str(name).title()+' Train')\n",
    "                \n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(key.replace('_',' ').title())\n",
    "        plt.legend()\n",
    "        plt.title(key)\n",
    "\n",
    "        plt.xlim([0,max(max(history.epoch) for (_, history) in histories)])\n",
    "\n",
    "plot_history([ \n",
    "    ('embedding', history)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T10:00:37.886Z"
    }
   },
   "outputs": [],
   "source": [
    "model = history.model\n",
    "embedding_vector_dict = {}\n",
    "embedding_vector_column = {}\n",
    "for feature in data_embedding_train.columns:\n",
    "    print(f' processing ... {feature}_embedding')\n",
    "    embedding_vector_dict[feature] = model.get_layer(f'{feature}_embedding').get_weights()[0]\n",
    "    embedding_vector_column[feature] = [f'{feature}_emb_{i}' for i in range(embedding_vector_dict[feature].shape[1])]\n",
    "print('Setup compelete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T10:00:38.086Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embedding_vector(feature, feature_label, embedding_vector_dict):\n",
    "    return embedding_vector_dict[feature][feature_label - 1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T10:00:38.273Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data_embedding_train = data_embedding_train.astype(int).reset_index(drop=True)\n",
    "data_embedding_val = data_embedding_val.astype(int).reset_index(drop=True)\n",
    "\n",
    "data_embedding_train_prepared = data_embedding_train.copy()\n",
    "data_embedding_val_prepared = data_embedding_val.copy()\n",
    "\n",
    "for feature in data_embedding_train.columns:\n",
    "    # train converting\n",
    "    tmp_train_features_df = pd.DataFrame(get_embedding_vector(feature, data_embedding_train[feature],\n",
    "                                                             embedding_vector_dict),\n",
    "                                             columns=embedding_vector_column[f'{feature}'])\n",
    "\n",
    "    # test converting\n",
    "    tmp_test_features_df = pd.DataFrame(get_embedding_vector(feature,data_embedding_val[feature],\n",
    "                                                            embedding_vector_dict),\n",
    "                                    columns=embedding_vector_column[f'{feature}'])\n",
    "    # train concat\n",
    "    data_embedding_train_prepared = pd.concat([data_embedding_train_prepared, tmp_train_features_df],\n",
    "                                               axis = 1)\n",
    "    # test concat\n",
    "    data_embedding_val_prepared = pd.concat([data_embedding_val_prepared, tmp_test_features_df],\n",
    "                                             axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T10:00:38.453Z"
    }
   },
   "outputs": [],
   "source": [
    "from pandas.core.common import flatten\n",
    "EMBEDDING_COLS = list(flatten(embedding_vector_column.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T10:00:38.614Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_logit_score = get_score(LogReg_model, \n",
    "                                  data_embedding_train_prepared[EMBEDDING_COLS],\n",
    "                                  y_train,\n",
    "                                  data_embedding_val_prepared[EMBEDDING_COLS],\n",
    "                                  y_val)\n",
    "embedding_xgb_score = get_score(xgb_model,\n",
    "                                data_embedding_train_prepared[EMBEDDING_COLS],\n",
    "                                y_train,\n",
    "                                data_embedding_val_prepared[EMBEDDING_COLS],\n",
    "                                y_val)\n",
    "print('Logistic Regression Embedding encoding:', embedding_logit_score)\n",
    "print('Xgboost Embedding encoding:', embedding_xgb_score)\n",
    "# # Embedding"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
